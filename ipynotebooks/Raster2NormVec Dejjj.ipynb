{
 "metadata": {
  "name": "Raster2NormVec Demo2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Demonstration of my code developed so far to prepare multiple remote sensing\n# time series for kohonen network analysis.\n\n# Import librariess"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import os\nimport pandas as pd\nfrom pandas import HDFStore\nimport gdal\nimport osr\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nimport numpy as np\nimport numpy.ma as ma\nimport decimal\nimport subprocess as sp\nimport numexpr\nimport bottleneck\nimport time",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Function to create lists of all raster inputs as Strings\n# Search subfolders to create List of input"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def rasterList(varpath, endwith, namefile):\n    # 1 - Precipitation\n    input_folder = varpath\n    varList = [os.path.join(root, name)\n                 for root, dirs, files in os.walk(input_folder)\n                 for name in files\n                 if name.endswith((endwith))]\n    print namefile + \" files : \"  + str(len(varList))\n    return varList",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# For each variable create a list with the input raster files\n# Print one list for checking purposes"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "prcpList = rasterList('I:/Data/TEST/prcp', \".tif\", \"Precipitation\")\ntmaxList = rasterList('I:/Data/TEST/tmax', \".tif\", \"Temp Max\")\ntminList = rasterList('I:/Data/TEST/tmin', \".tif\", \"Temp Min\")\nndviList = rasterList('I:/Data/TEST/ndvi', \".tif\", \"NDVI\")\nlstmList = rasterList('I:/Data/TEST/lst', \".tif\", \"Land surface temp\")\nevtmList = rasterList('I:/Data/TEST/evtm', \".tif\", \"Evaporation\")\nprint prcpList",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Function to open raster image by providing filepath used to define the\n# Subsets for the grouping of variables"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def SubsetImg(nameras):\n    # Open landcover image\n    a = gdal.Open(nameras, gdal.GA_ReadOnly)\n    b = a.GetRasterBand(1)\n    InRas = b.ReadAsArray()\n    return InRas",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Define a function that can visualise the output of the selected parts\n# Of the landcover map by using basemap library."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def plot_gdal_file ( input_dataset, x, vmin=0, vmax=100 ):\n    # Need to get the geotransform (ie, corners)\n    geo = input_dataset.GetGeoTransform()\n    data = x\n\n    # We need to flip the raster upside down\n    data = np.flipud( data )\n\n    # Define a cylindrical projection\n    projection_opts={'projection':'cyl','resolution':'h'}\n\n    # These are the extents in the native raster coordinates\n    extent = [ geo[0],  geo[0] + input_dataset.RasterXSize*geo[1], \\\n        geo[3], geo[3] + input_dataset.RasterYSize*geo[5]]\n    map = Basemap( llcrnrlon=extent[0], llcrnrlat=extent[3], \\\n                 urcrnrlon=extent[1],urcrnrlat=extent[2],  ** projection_opts)\n\n    # Set up some variables of the map presentation\n    cmap = plt.cm.gist_rainbow\n    cmap.set_under ('0.8' )\n    cmap.set_bad('0.8' )\n    cmap.set_over ('0.8')\n\n    # Set range colourbar, draw country borders\n    map.imshow( data, vmin=vmin, vmax=vmax, cmap=cmap, interpolation='nearest')\n    map.drawcoastlines (linewidth=0.5, color='k')\n    map.drawcountries(linewidth=0.5, color='k')\n    map.drawmeridians( np.arange(0,360,5), color='k')\n    map.drawparallels( np.arange(-90,90,5), color='k')\n    map.drawmapboundary()\n\n    cb = plt.colorbar( orientation='horizontal', fraction=0.10, shrink=0.8)\n\n    # Add custom title to map\n    plt.title('Landcovertype '+str(i)+' (subset contains: '+str(z)+' values)')\n    plt.show()",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Load the landcover raster map and create raster file with shape of Landcover\n# raster map to be able to plot the subsets. Further plot original image."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "nameras = r'I:/Data/landcover/raw_all_lc_resample.tif'\nInRas = SubsetImg(nameras)\na = gdal.Open(nameras, gdal.GA_ReadOnly)\nx = np.empty(InRas.shape); x[:] = 127\ni = '0 - 16'\nz = InRas.shape[0]*InRas.shape[1]\nplot_gdal_file(a,InRas,vmin=0,vmax=17) # Return landcover dataset",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Function raster2hdf: input listvar = list: name of raster for the\n# Specific variable, namevar = 'string': name of column for HDF5 table"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def raster2hdf(listvar, namevar):\n    for raster in listvar:\n\n        c = gdal.Open(raster, gdal.GA_ReadOnly)\n        d = c.GetRasterBand(1)\n        r_data = d.ReadAsArray()\n        root, ext = os.path.splitext(raster)\n        name = int(decimal.Decimal(os.path.basename(root)))\n\n        # Extract real values from array by compressing the masked items\n        # Convert numpy array to DataFrame and append to HDF5 table\n        Arr_df = pd.DataFrame(dict(x=indices[0],\n                                   y=indices[1],\n                                   date=name,\n                                   value=r_data[indices].astype('float'))\n                              ).set_index(['x','y','date'])\n        Arr_df.rename(index=None, columns={'value':namevar}, inplace=True)\n        store.append(namevar,Arr_df)\n\n        # Clean up my dirt\n        c = None\n        d = None\n        r_data = None\n        Arr_extr = None\n        Arr_df = None\n        Arr_x = None\n        Arr_y = None\n\n    return store",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Iterate over the different landcover types and extract the area of the\n# Different remote sensing products equal to the landcover type. Store both\n# values as indices in HDFStore."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "for i in range(2,3):\n    # Create HDF5 store files\n    outFile ='I:/Data/output/demo2\\\\'+str(i)+'.h5'\n    print outFile\n    store = HDFStore(outFile, mode='a')\n\n    # Mask everything what is not equal to the current landcover type\n    # Return number of elements of unmasked items for dimensions numpy array\n    indices = np.where(InRas==i)\n\n    x[indices] = i\n    z = np.sum(x==i)\n    plot_gdal_file(a,x,vmin=0,vmax=17) # Return plot to visualise mask\n    x[:] = 127\n\n    # 1 - Precipitation\n    print \"Precipitation................ OK\"\n    raster2hdf(prcpList,'prcp')\n    # 2 - Temperature Maximum\n    print \"TMax......................... OK\"\n    raster2hdf(tmaxList,'tmax')\n    # 3 - Temperature Minimum\n    print \"TMin......................... OK\"\n    raster2hdf(tminList,'tmin')\n    # 4 - NDVI\n    print \"NDVI......................... OK\"\n    raster2hdf(ndviList,'ndvi')\n    # 5 - Land surface temperature\n    print \"Land surface temp............ OK\"\n    raster2hdf(lstmList,'lstm')\n    # 6 - Evapotranspiration\n    print \"Evapotranspiration........... OK\"\n    raster2hdf(evtmList,'evtm')\n    print store",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Next few lines of code returns the head of the different nodes saved in the\n# HDFStore"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print store['evtm'].head(2)\nprint store['lstm'].head(2)\nprint store['ndvi'].head(2)\nprint store['prcp'].head(2)\nprint store['tmax'].head(2)\nprint store['tmin'].head(2)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Now we've converted and appended the data of the remote sensing products into\n# Different HDFStores. But next step is to do an inner merge on the data values\n# to obtain a new node to be used as input for the kohonen analysis. Let's\n# start with creating a list of the .h5 files in the folder used for conversion."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "hdfList = rasterList('I:/Data/output/demo2', '.h5','HDFStore')\nA = store",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# We will only demonstrate the code for the file instead of using a for loop.\n# Firstly create a link to the store containing the data and create a\n# temporarily HDFStore."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "h5 = hdfList[0]\nh5temp = h5.replace('.h5','temp.h5')\nA = HDFStore(h5)\nAtemp = HDFStore(h5temp)\nprint A\nprint Atemp",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Then we set the chunk size and get the number of rows of the two nodes to be\n# able to decide the start and stop moments of each chunk"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "nrows_a = A.get_storer('evtm').nrows\nnrows_b = A.get_storer('lstm').nrows\na_chunk_size = 100\nb_chunk_size = 100",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Now we can merge the two nodes together with the following code. The merged\n# nodes will be stored in the temporarily store Atemp."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "for a in xrange(int(nrows_a / a_chunk_size) + 1):\n    a_start_i = a * a_chunk_size\n    a_stop_i  = min((a + 1) * a_chunk_size, nrows_a)\n    print '** evtm rows '+str(a_start_i)+' - '+str(a_stop_i)+' **'\n    a = A.select('evtm', start = a_start_i, stop = a_stop_i)\n    for b in xrange(int(nrows_b / b_chunk_size) + 1):\n        b_start_i = b * b_chunk_size\n        b_stop_i = min((b + 1) * b_chunk_size, nrows_b)\n        print 'lstm rows '+str(b_start_i)+' - '+str(b_stop_i)\n        b = A.select('lstm', start = b_start_i, stop = b_stop_i)\n        try:\n            df = pd.merge(a, b , left_index=True, right_index=True,how='inner')\n        except Exception as err:\n            print \"no merge possible between rows, but let's continue\"\n            df = []\n        if len(df):\n            Atemp.append('el', df)",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Since we want to merge all the different remote sensing products set up a\n# function which assist us."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def mergeVarAAtemp(col_in1,col_in2,col_out):\n    print 'merging '+col_in1+' with '+col_in2+' to '+col_out\n    nrows_a = A.get_storer(col_in1).nrows\n    nrows_b = Atemp.get_storer(col_in2).nrows\n    a_chunk_size = 100\n    b_chunk_size = 100\n\n    for a in xrange(int(nrows_a / a_chunk_size) + 1):\n        a_start_i = a * a_chunk_size\n        a_stop_i  = min((a + 1) * a_chunk_size, nrows_a)\n        print '* '+str(col_in1)+' rows '+str(a_start_i)+' - '+str(a_stop_i)+' *'\n        a = A.select(col_in1, start = a_start_i, stop = a_stop_i)\n        for b in xrange(int(nrows_b / b_chunk_size) + 1):\n            b_start_i = b * b_chunk_size\n            b_stop_i = min((b + 1) * b_chunk_size, nrows_b)\n            print str(col_in2)+' rows '+str(b_start_i)+' - '+str(b_stop_i)\n            b = Atemp.select(col_in2, start = b_start_i, stop = b_stop_i)\n            try:\n                df = pd.merge(a, b , left_index=True, right_index=True,\n                              how='inner')\n            except Exception as err:\n                print \"no merge possible between rows, but let's continue\"\n                ##df = pd.merge(a, b , left_index=True, right_index=True, how='outer').dropna()\n                df = []\n            if len(df):\n                Atemp.append(col_out, df)\n    return\n\nmergeVarAAtemp('ndvi','el','eln')\ndel Atemp['el']\nmergeVarAAtemp('prcp','eln','elnp')\ndel Atemp['eln']\nmergeVarAAtemp('tmax','elnp','elnpt')\ndel Atemp['elnp']\nmergeVarAAtemp('tmin','elnpt','elnptt')\ndel Atemp['elnpt']\nprint \"Merging data columns......... OK\"",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# First an overview of the head of the inner merged nodes."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "Atemp['elnptt'].head()\nAtemp['elnptt'].dtypes",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Need to further massage the data to add a index on the right starting with\n# one. Since the HDFStore is row-oriented it's not possible to add columns to\n# existing nodes. Furthermore the index should be created lazily to avoid a\n# Memory Error. The index created is based on the selection that is merged.\n# Final product is stored in the original HDFStore and the temporarily HDFStore\n# will be deleted."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "print \"Reset index.................. OK\"\nfor chunk in Atemp.select('elnptt', chunksize=100):\n    df = chunk.reset_index()\n    try:\n        nrows = Atemp.get_storer('elnpttnoin').nrows\n    except:\n        nrows = 0\n    df.index = pd.Series(df.index) + nrows\n    Atemp.append('elnpttnoin',df)\n\ndel Atemp['elnptt']\n\nprint \"Add right index.............. OK\"\nAtemp.append('data_column', pd.DataFrame(pd.Series(xrange(1,\n         len(Atemp.root.elnpttnoin.table)+1),\n         dtype='int32'),columns=['indx']), data_column=True)\n\n\nprint \"Multiple node select......... OK\"\nfor chunk in Atemp.select_as_multiple(['elnpttnoin','data_column'],\n                                      chunksize=100):\n    Atemp.append('elnpttnoindc',chunk)\ndel Atemp['data_column']\ndel Atemp['elnpttnoin']\n\n\nprint \"Set index.................... OK\"\nprint \"From temp to original store.. OK\"\nfor chunk in Atemp.select('elnpttnoindc', chunksize=100):\n    chunk[['tmin','tmax','prcp']] = chunk[['tmin','tmax','prcp']].astype(float)\n    chunk[['ndvi','evtm','lstm']] = chunk[['ndvi','evtm','lstm']].astype(int)\n    A.append('data_index', chunk.set_index(['x','y','date']))\ndel Atemp['elnpttnoindc']\n\nprint \"Remove temporarily store..... OK\"\nprint Atemp\nAtemp.close()\nos.remove(h5temp)\nprint A['data_index']",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Finally we've to create a .vec file that has a header containing some general\n# information."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "outVec = h5.replace('.h5','.vec')\na = np.asarray([['$TYPE vec'],\n                ['$XDIM '+str(len(A.root.data_index.table))],\n                ['$YDIM 1'],\n                ['$VEC_DIM 6']\n               ])\nnp.savetxt(outVec,a, fmt='%s')",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# And accordingly we append the data chunk by chunk to the .vec file and close\n# the door behind us."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "for chunk in A.select('data_index', chunksize=100):\n    chunk.to_csv(outVec, sep=' ', header=False, index=False, mode='ab')\nA.close()\nprint \"Converting 2 vec file........ OK\"",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Next step is to normalise the values based on the standard deviation and to\n# train the values as a unsupervised neural network mdoel also known as the\n# Self-Organising Map. We use the software Java SOMToolbox developed at the\n# Institute of Software Technology and Interactive System at the Vienna\n# University of Technology."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# Normalisation of the data\noutVecNorm = outVec.replace('.vec','_norm.vec')\nbat = r\"C:\\Program Files\\SOMToolbox\\somtoolbox.bat\"\nparamsnorm = [bat, \"SOMLibVectorNormalization\", \"-m\", \"STANDARD_SCORE\",\n              outVec, outVecNorm]\nprint sp.list2cmdline(paramsnorm)\nnorm = sp.Popen(sp.list2cmdline(paramsnorm))\nprint \"Data normalisation........... OK\"",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Training of the data if files exist"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def train():\n    paramstrain = [bat, \"GrowingSOM\", r\"I:\\Data\\output\\demo2\\som.prop\"]\n    print sp.list2cmdline(paramstrain)\n    train = sp.Popen(sp.list2cmdline(paramstrain))\n    print \"Data training................ OK\"\n\ntrain()\nif os.path.isfile(outVecNorm):\n    train()\nelse:\n    print 'no files to train'",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "# Viewing of the data"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "def view():\n    paramsview = [bat, \"SOMViewer\", '-u',\n                  r\"I:\\Data\\output\\demo2\\SOM\\demo.unit.gz\", '-w',\n                  r\"I:\\Data\\output\\demo2\\SOM\\demo.wgt.gz\",'--dw',\n                  r\"I:\\Data\\output\\demo2\\SOM\\demo.dwm.gz\"]\n    print sp.list2cmdline(paramsview)\n    view = sp.Popen(sp.list2cmdline(paramsview))\n    print \"Trying to view data.......... OK\"\n\nprint 'traing of data is in progress..'\n\ntime.sleep(3)\n\nif os.path.isfile(r\"I:\\Data\\output\\demo2\\SOM\\demo.dwm.gz\"):\n    view()\nelse:\n    print 'no files to view'",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}