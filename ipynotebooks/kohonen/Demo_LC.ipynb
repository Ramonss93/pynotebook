{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Demonstration of my code developed so far to prepare multiple remote sensing time series for kohonen network analysis.\n",
      "\n",
      "First import libraries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import pandas as pd\n",
      "from pandas import HDFStore\n",
      "from osgeo import gdal\n",
      "#import osr\n",
      "import matplotlib.pyplot as plt\n",
      "#from mpl_toolkits.basemap import Basemap\n",
      "import numpy as np\n",
      "import numpy.ma as ma\n",
      "import decimal\n",
      "import subprocess as sp\n",
      "import numexpr\n",
      "import bottleneck\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "List of all strings used in code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prcpFiles = r'I:/Data/TEST/prcp'\n",
      "tmaxFiles = r'I:/Data/TEST/tmax'\n",
      "tminFiles = r'I:/Data/TEST/tmin'\n",
      "ndviFiles = r'I:/Data/TEST/ndvi'\n",
      "lstmFiles = r'I:/Data/TEST/lst'\n",
      "evtmFiles = r'I:/Data/TEST/evtm'\n",
      "maskRastr = r'I:/Data/landcover/raw_all_lc_Resample.tif'\n",
      "outpFiles = r'I:/Data/output/demo2/'\n",
      "somSource = r'C:/Program Files/SOMToolbox/somtoolbox.bat'\n",
      "fromMask = int(2)\n",
      "toMask = int(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lstmFiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "'I:/Data/TEST/lst'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Function to create lists of all raster inputs as Strings.\n",
      "Search subfolders to create List of input"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rasterList(varpath, endwith, namefile):\n",
      "    # 1 - Precipitation\n",
      "    input_folder = varpath\n",
      "    varList = [os.path.join(root, name)\n",
      "                 for root, dirs, files in os.walk(input_folder)\n",
      "                 for name in files\n",
      "                 if name.endswith((endwith))]\n",
      "    print (namefile + \" files : \"  + str(len(varList)))\n",
      "    return varList"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "For each variable create a list with the input raster files.\n",
      "Print one list for checking purposes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prcpList = rasterList(prcpFiles, \".tif\", \"Precipitation\")\n",
      "tmaxList = rasterList(tmaxFiles, \".tif\", \"Temp Max\")\n",
      "tminList = rasterList(tminFiles, \".tif\", \"Temp Min\")\n",
      "ndviList = rasterList(ndviFiles, \".tif\", \"NDVI\")\n",
      "lstmList = rasterList(lstmFiles, \".tif\", \"Land surface temp\")\n",
      "evtmList = rasterList(evtmFiles, \".tif\", \"Evaporation\")\n",
      "print (prcpList)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Precipitation files : 0\n",
        "Temp Max files : 0\n",
        "Temp Min files : 0\n",
        "NDVI files : 0\n",
        "Land surface temp files : 0\n",
        "Evaporation files : 0\n",
        "[]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Function to open raster image by providing filepath used to define the \n",
      "subsets for the grouping of variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def SubsetImg(nameras):\n",
      "    # Open landcover image\n",
      "    a = gdal.Open(nameras, gdal.GA_ReadOnly)\n",
      "    b = a.GetRasterBand(1)\n",
      "    InRas = b.ReadAsArray()\n",
      "    return InRas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Define a function that can visualise the output of the selected parts\n",
      "of the landcover map by using basemap library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_gdal_file ( input_dataset, x, vmin=0, vmax=100 ):\n",
      "    # Need to get the geotransform (ie, corners)\n",
      "    geo = input_dataset.GetGeoTransform()\n",
      "    data = x\n",
      "\n",
      "    # We need to flip the raster upside down\n",
      "    data = np.flipud( data )\n",
      "\n",
      "    # Define a cylindrical projection\n",
      "    projection_opts={'projection':'cyl','resolution':'h'}\n",
      "\n",
      "    # These are the extents in the native raster coordinates\n",
      "    extent = [ geo[0],  geo[0] + input_dataset.RasterXSize*geo[1], \\\n",
      "        geo[3], geo[3] + input_dataset.RasterYSize*geo[5]]\n",
      "    map = Basemap( llcrnrlon=extent[0], llcrnrlat=extent[3], \\\n",
      "                 urcrnrlon=extent[1],urcrnrlat=extent[2],  ** projection_opts)\n",
      "\n",
      "    # Set up some variables of the map presentation\n",
      "    cmap = plt.cm.gist_rainbow\n",
      "    cmap.set_under ('0.8' )\n",
      "    cmap.set_bad('0.8' )\n",
      "    cmap.set_over ('0.8')\n",
      "\n",
      "    # Set range colourbar, draw country borders\n",
      "    map.imshow( data, vmin=vmin, vmax=vmax, cmap=cmap, interpolation='nearest')\n",
      "    map.drawcoastlines (linewidth=0.5, color='k')\n",
      "    map.drawcountries(linewidth=0.5, color='k')\n",
      "    map.drawmeridians( np.arange(0,360,5), color='k')\n",
      "    map.drawparallels( np.arange(-90,90,5), color='k')\n",
      "    map.drawmapboundary()\n",
      "\n",
      "    cb = plt.colorbar( orientation='horizontal', fraction=0.10, shrink=0.8)\n",
      "\n",
      "    # Add custom title to map\n",
      "    plt.title('Landcovertype '+str(i)+' (subset contains: '+str(z)+' values)')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Load the landcover raster map and create raster file with shape of Landcover\n",
      "raster map to be able to plot the subsets. Further plot original image."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nameras = maskRastr\n",
      "InRas = SubsetImg(nameras)\n",
      "a = gdal.Open(nameras, gdal.GA_ReadOnly)\n",
      "x = np.empty(InRas.shape); x[:] = 127\n",
      "i = '0 - 16'\n",
      "z = InRas.shape[0]*InRas.shape[1]\n",
      "plot_gdal_file(a,InRas,vmin=0,vmax=17) # Return landcover dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Function raster2hdf: input listvar = list: name of raster for the\n",
      "specific variable, namevar = 'string': name of column for HDF5 table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def raster2hdf(listvar, namevar):\n",
      "    for raster in listvar:\n",
      "\n",
      "        c = gdal.Open(raster, gdal.GA_ReadOnly)\n",
      "        d = c.GetRasterBand(1)\n",
      "        r_data = d.ReadAsArray()\n",
      "        root, ext = os.path.splitext(raster)\n",
      "        name = int(decimal.Decimal(os.path.basename(root)))\n",
      "\n",
      "        # Extract real values from array by compressing the masked items\n",
      "        # Convert numpy array to DataFrame and append to HDF5 table\n",
      "        Arr_df = pd.DataFrame(dict(x=indices[0],\n",
      "                                   y=indices[1],\n",
      "                                   date=name,\n",
      "                                   value=r_data[indices].astype('float'))\n",
      "                              ).set_index(['x','y','date'])\n",
      "        Arr_df.rename(index=None, columns={'value':namevar}, inplace=True)\n",
      "        store.append(namevar,Arr_df)\n",
      "\n",
      "        # Clean up my dirt\n",
      "        c = None\n",
      "        d = None\n",
      "        r_data = None\n",
      "        Arr_extr = None\n",
      "        Arr_df = None\n",
      "        Arr_x = None\n",
      "        Arr_y = None\n",
      "\n",
      "    return store"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Iterate over the different landcover types and extract the area of the\n",
      "Different remote sensing products equal to the landcover type. Store both\n",
      "values as indices in HDFStore."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(fromMask,toMask):\n",
      "    # Create HDF5 store files\n",
      "    outFile =outpFiles+str(i)+'.h5'\n",
      "    print outFile\n",
      "    store = HDFStore(outFile, mode='a')\n",
      "\n",
      "    # Mask everything what is not equal to the current landcover type\n",
      "    # Return number of elements of unmasked items for dimensions numpy array\n",
      "    indices = np.where(InRas==i)\n",
      "\n",
      "    x[indices] = i\n",
      "    z = np.sum(x==i)\n",
      "    plot_gdal_file(a,x,vmin=0,vmax=17) # Return plot to visualise mask\n",
      "    x[:] = 127\n",
      "\n",
      "    # 1 - Precipitation\n",
      "    print \"Precipitation................ OK\"\n",
      "    raster2hdf(prcpList,'prcp')\n",
      "    # 2 - Temperature Maximum\n",
      "    print \"TMax......................... OK\"\n",
      "    raster2hdf(tmaxList,'tmax')\n",
      "    # 3 - Temperature Minimum\n",
      "    print \"TMin......................... OK\"\n",
      "    raster2hdf(tminList,'tmin')\n",
      "    # 4 - NDVI\n",
      "    print \"NDVI......................... OK\"\n",
      "    raster2hdf(ndviList,'ndvi')\n",
      "    # 5 - Land surface temperature\n",
      "    print \"Land surface temp............ OK\"\n",
      "    raster2hdf(lstmList,'lstm')\n",
      "    # 6 - Evapotranspiration\n",
      "    print \"Evapotranspiration........... OK\"\n",
      "    raster2hdf(evtmList,'evtm')\n",
      "    print store"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Next few lines of code returns the head of the different nodes saved in the\n",
      "HDFStore"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print store['evtm'].head(2)\n",
      "print store['lstm'].head(2)\n",
      "print store['ndvi'].head(2)\n",
      "print store['prcp'].head(2)\n",
      "print store['tmax'].head(2)\n",
      "print store['tmin'].head(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Now we've converted and appended the data of the remote sensing products into\n",
      "Different HDFStores. But next step is to do an inner merge on the data values\n",
      "to obtain a new node to be used as input for the kohonen analysis. Let's\n",
      "start with creating a list of the .h5 files in the folder used for conversion."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hdfList = rasterList(outpFiles, '.h5','HDFStore')\n",
      "A = store"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "We will only demonstrate the code for the file instead of using a for loop.\n",
      "Firstly create a link to the store containing the data and create a\n",
      "temporarily HDFStore."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h5 = hdfList[0]\n",
      "h5temp = h5.replace('.h5','temp.h5')\n",
      "A = HDFStore(h5)\n",
      "Atemp = HDFStore(h5temp)\n",
      "print A\n",
      "print Atemp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Then we set the chunk size and get the number of rows of the two nodes to be\n",
      "able to decide the start and stop moments of each chunk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nrows_a = A.get_storer('evtm').nrows\n",
      "nrows_b = A.get_storer('lstm').nrows\n",
      "a_chunk_size = 100000\n",
      "b_chunk_size = 100000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Now we can merge the two nodes together with the following code. The merged\n",
      "nodes will be stored in the temporarily store Atemp."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for a in xrange(int(nrows_a / a_chunk_size) + 1):\n",
      "    a_start_i = a * a_chunk_size\n",
      "    a_stop_i  = min((a + 1) * a_chunk_size, nrows_a)\n",
      "    print '** evtm rows '+str(a_start_i)+' - '+str(a_stop_i)+' **'\n",
      "    a = A.select('evtm', start = a_start_i, stop = a_stop_i)\n",
      "    for b in xrange(int(nrows_b / b_chunk_size) + 1):\n",
      "        b_start_i = b * b_chunk_size\n",
      "        b_stop_i = min((b + 1) * b_chunk_size, nrows_b)\n",
      "        print 'lstm rows '+str(b_start_i)+' - '+str(b_stop_i)\n",
      "        b = A.select('lstm', start = b_start_i, stop = b_stop_i)\n",
      "        try:\n",
      "            df = pd.merge(a, b , left_index=True, right_index=True,how='inner')\n",
      "        except Exception as err:\n",
      "            print \"no merge possible between rows, but let's continue\"\n",
      "            df = []\n",
      "        if len(df):\n",
      "            Atemp.append('el', df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Since we want to merge all the different remote sensing products set up a\n",
      "function which assist us."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mergeVarAAtemp(col_in1,col_in2,col_out):\n",
      "    print 'merging '+col_in1+' with '+col_in2+' to '+col_out\n",
      "    nrows_a = A.get_storer(col_in1).nrows\n",
      "    nrows_b = Atemp.get_storer(col_in2).nrows\n",
      "    a_chunk_size = 100000\n",
      "    b_chunk_size = 100000\n",
      "\n",
      "    for a in xrange(int(nrows_a / a_chunk_size) + 1):\n",
      "        a_start_i = a * a_chunk_size\n",
      "        a_stop_i  = min((a + 1) * a_chunk_size, nrows_a)\n",
      "        print '* '+str(col_in1)+' rows '+str(a_start_i)+' - '+str(a_stop_i)+' *'\n",
      "        a = A.select(col_in1, start = a_start_i, stop = a_stop_i)\n",
      "        for b in xrange(int(nrows_b / b_chunk_size) + 1):\n",
      "            b_start_i = b * b_chunk_size\n",
      "            b_stop_i = min((b + 1) * b_chunk_size, nrows_b)\n",
      "            print str(col_in2)+' rows '+str(b_start_i)+' - '+str(b_stop_i)\n",
      "            b = Atemp.select(col_in2, start = b_start_i, stop = b_stop_i)\n",
      "            try:\n",
      "                df = pd.merge(a, b , left_index=True, right_index=True,\n",
      "                              how='inner')\n",
      "            except Exception as err:\n",
      "                print \"no merge possible between rows, but let's continue\"\n",
      "                ##df = pd.merge(a, b , left_index=True, right_index=True, how='outer').dropna()\n",
      "                df = []\n",
      "            if len(df):\n",
      "                Atemp.append(col_out, df)\n",
      "    return\n",
      "\n",
      "mergeVarAAtemp('ndvi','el','eln')\n",
      "del Atemp['el']\n",
      "mergeVarAAtemp('prcp','eln','elnp')\n",
      "del Atemp['eln']\n",
      "mergeVarAAtemp('tmax','elnp','elnpt')\n",
      "del Atemp['elnp']\n",
      "mergeVarAAtemp('tmin','elnpt','elnptt')\n",
      "del Atemp['elnpt']\n",
      "print \"Merging data columns......... OK\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "First an overview of the head of the inner merged nodes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Atemp['elnptt'].head(20)\n",
      "print Atemp['elnptt'].dtypes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Need to further massage the data to add a index on the right starting with\n",
      "one. Since the HDFStore is row-oriented it's not possible to add columns to\n",
      "existing nodes. Furthermore the index should be created lazily to avoid a\n",
      "Memory Error (which does'nt work with big datasets, need something different). The index created is based on the selection that is merged.\n",
      "Final product is stored in the original HDFStore and the temporarily HDFStore\n",
      "will be deleted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Reset index.................. OK\"\n",
      "for chunk in Atemp.select('elnptt', chunksize=100000):\n",
      "    df = chunk.reset_index()\n",
      "    try:\n",
      "        nrows = Atemp.get_storer('elnpttnoin').nrows\n",
      "    except:\n",
      "        nrows = 0\n",
      "    df.index = pd.Series(df.index) + nrows\n",
      "    Atemp.append('elnpttnoin',df)\n",
      "\n",
      "del Atemp['elnptt']\n",
      "\n",
      "print \"Add right index.............. OK\"\n",
      "Atemp.append('data_column', pd.DataFrame(pd.Series(xrange(1,\n",
      "         len(Atemp.root.elnpttnoin.table)+1),\n",
      "         dtype='int32'),columns=['indx']), data_column=True)\n",
      "\n",
      "\n",
      "print \"Multiple node select......... OK\"\n",
      "for chunk in Atemp.select_as_multiple(['elnpttnoin','data_column'],\n",
      "                                      chunksize=100000):\n",
      "    Atemp.append('elnpttnoindc',chunk)\n",
      "del Atemp['data_column']\n",
      "del Atemp['elnpttnoin']\n",
      "\n",
      "\n",
      "print \"Set index.................... OK\"\n",
      "print \"From temp to original store.. OK\"\n",
      "for chunk in Atemp.select('elnpttnoindc', chunksize=100000):\n",
      "    chunk[['tmin','tmax','prcp']] = chunk[['tmin','tmax','prcp']].astype(float)\n",
      "    chunk[['ndvi','evtm','lstm']] = chunk[['ndvi','evtm','lstm']].astype(int)\n",
      "    A.append('data_index', chunk.set_index(['x','y','date']))\n",
      "del Atemp['elnpttnoindc']\n",
      "\n",
      "print \"Remove temporarily store..... OK\"\n",
      "print Atemp\n",
      "Atemp.close()\n",
      "os.remove(h5temp)\n",
      "print A['data_index']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Finally we've to create a .vec file that has a header containing some general\n",
      "information."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outVec = h5.replace('.h5','.vec')\n",
      "a = np.asarray([['$TYPE vec'],\n",
      "                ['$XDIM '+str(len(A.root.data_index.table))],\n",
      "                ['$YDIM 1'],\n",
      "                ['$VEC_DIM 6']\n",
      "               ])\n",
      "np.savetxt(outVec,a, fmt='%s')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "And accordingly we append the data chunk by chunk to the .vec file and close\n",
      "the door behind us."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for chunk in A.select('data_index', chunksize=100000):\n",
      "    chunk.to_csv(outVec, sep=' ', header=False, index=False, mode='ab')\n",
      "A.close()\n",
      "print \"Converting 2 vec file........ OK\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Next step is to normalise the values based on the standard deviation and to\n",
      "train the values as a unsupervised neural network mdoel also known as the\n",
      "Self-Organising Map. We use the software Java SOMToolbox developed at the\n",
      "Institute of Software Technology and Interactive System at the Vienna\n",
      "University of Technology."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Normalisation of the data\n",
      "outVecNorm = outVec.replace('.vec','_norm.vec')\n",
      "bat = somSource\n",
      "paramsnorm = [bat, \"SOMLibVectorNormalization\", \"-m\", \"STANDARD_SCORE\",\n",
      "              outVec, outVecNorm]\n",
      "print sp.list2cmdline(paramsnorm)\n",
      "norm = sp.Popen(sp.list2cmdline(paramsnorm))\n",
      "print \"Data normalisation........... OK\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Training of the data if files exist"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train():\n",
      "    paramstrain = [bat, \"GrowingSOM\", outpFiles+\"som.prop\"]\n",
      "    print sp.list2cmdline(paramstrain)\n",
      "    train = sp.Popen(sp.list2cmdline(paramstrain))\n",
      "    print \"Data training................ OK\"\n",
      "\n",
      "if os.path.isfile(outVecNorm):\n",
      "    train()\n",
      "else:\n",
      "    print 'no files to train'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Viewing of the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def view():\n",
      "    paramsview = [bat, \"SOMViewer\", '-u',\n",
      "                  outpFiles+\"SOM\\demo.unit.gz\", '-w',\n",
      "                  outpFiles+\"SOM\\demo.wgt.gz\",'--dw',\n",
      "                  outpFiles+\"SOM\\demo.dwm.gz\"]\n",
      "    print sp.list2cmdline(paramsview)\n",
      "    view = sp.Popen(sp.list2cmdline(paramsview))\n",
      "    print \"Trying to view data.......... OK\"\n",
      "\n",
      "print 'training of data is in progress..'\n",
      "\n",
      "if os.path.isfile(outpFiles+\"SOM\\demo.dwm.gz\"):\n",
      "    view()\n",
      "else:\n",
      "    print 'no files to view'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Several different concept visualisation outputs for illustration purposes only. It's just testing of the different possibilties. Images are from today 24/7/2013. Different clusters are cleary visible. In the image with overview of all 6 variables gives nice overview. Even though it's not reliable since evtm file contains some very high values for urban areas, which should be filtered, for this reason the normalisation is biased and this results in a predominantly red map for the 'normal' values, with the biased high values as blue in the bottom left corner. Last six maps are the different component planes individually, with different colours. No labels are given but they are from left to right, top to bottom, tmin, tmax, prcp, ndvi, evtm, and lstm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/1.png\">\n",
      "<img src=\"files/2.png\">\n",
      "<img src=\"files/3.png\">\n",
      "<img src=\"files/4.png\">\n",
      "<img src=\"files/5.png\">\n",
      "<img src=\"files/6.png\">\n",
      "<img src=\"files/7tmin.png\">\n",
      "<img src=\"files/8tmax.png\">\n",
      "<img src=\"files/9prcp.png\">\n",
      "<img src=\"files/10ndvi.png\">\n",
      "<img src=\"files/11evtm.png\">\n",
      "<img src=\"files/12lstm.png\">"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}